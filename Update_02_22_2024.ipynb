{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KDRuPNco2k04",
        "OlXYonOh8mfm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeDongN/Medica_AI2/blob/main/Update_02_22_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code read a folder of data. After applying preprocessing (DC component removal), it will extract number of maximas specified by a number and the labels will be assigned through final_function. The data is then balanced by ADASYN and SMOTE oversampling methods. At the end, Multiple models are used as classification and regression approaches.\n",
        "1)KNN, Decision Tree, Random Forest, Gradiant Boosting. The accuracies will be compared with confusion matrices at the end."
      ],
      "metadata": {
        "id": "E6AtWSMa7Gwi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d5hWJjhaSo5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4CmlDwMjjj9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae51e3a0-4b0e-4793-cf19-92e29f162a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import sklearn\n",
        "from google.colab import drive\n",
        "from scipy.signal import find_peaks\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/new_data/1stSeries_07.05.21'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# functions"
      ],
      "metadata": {
        "id": "qfyQqOjXkoUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### importing first data function"
      ],
      "metadata": {
        "id": "KDRuPNco2k04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_data_import():\n",
        "  # import the first_data\n",
        "  # first_data making\n",
        "  first_data = []\n",
        "  first_data = pd.read_csv('/content/drive/My Drive/data2/REF.CSV', skiprows=24)\n",
        "  first_data_origin = pd.read_csv('/content/drive/My Drive/data2/REF.CSV', skiprows=24)\n",
        "  first_a = float(first_data_origin.columns[0])\n",
        "  first_b = float(first_data_origin.columns[1])\n",
        "  first_data.loc[0] = [first_a, first_b]\n",
        "  first_data.columns=['Wave_length', 'Amplitude']\n",
        "  first_data['label']=1.3\n",
        "  first_np = first_data[['Wave_length', 'Amplitude']].to_numpy()\n",
        "  first_np = np.ravel(first_np, order='F')\n",
        "  first_np = np.array([first_np])\n",
        "\n",
        "  return first_data, first_np"
      ],
      "metadata": {
        "id": "upWEq9PEAww5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### importing whole data and stack function"
      ],
      "metadata": {
        "id": "iz_Oa8kW1l0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def stack_data(path):\n",
        "    stack_np = None  # Initialize as None to properly concatenate later\n",
        "    folder_list = os.listdir(path)\n",
        "\n",
        "    for folder_name in folder_list:\n",
        "        temp_folder_list = os.listdir(os.path.join(path, folder_name))\n",
        "\n",
        "        for file_name in temp_folder_list:\n",
        "            # Read the data into a Pandas DataFrame\n",
        "            file_path = os.path.join(path, folder_name, file_name)\n",
        "            data = pd.read_csv(file_path, skiprows=24)\n",
        "\n",
        "            # Extract the first two columns and rename them\n",
        "            data = data.iloc[:, :2]\n",
        "            data.columns = ['Wave_length', 'Amplitude']\n",
        "\n",
        "            # Stack the data as a single row\n",
        "            temp = data.values.ravel(order='F')  # Convert DataFrame to NumPy array and flatten\n",
        "            temp = temp.reshape(1, -1)  # Reshape to a 2D array with a single row\n",
        "\n",
        "            # Concatenate to stack_np\n",
        "            if stack_np is None:\n",
        "                stack_np = temp\n",
        "            else:\n",
        "                stack_np = np.concatenate((stack_np, temp), axis=1)\n",
        "\n",
        "    return stack_np\n"
      ],
      "metadata": {
        "id": "YuVeX1GIBMOh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### removing DC function"
      ],
      "metadata": {
        "id": "ytv5ry9u1ZCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing a simple DC removal function\n",
        "def remove_dc(signal):\n",
        "    # Calculate the mean of the signal\n",
        "    signal_mean = np.mean(signal)\n",
        "    # Remove the mean from the signal\n",
        "    dc_removed_signal = signal - signal_mean\n",
        "    return dc_removed_signal"
      ],
      "metadata": {
        "id": "nJrTjRgYmB--"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### finding peaks function"
      ],
      "metadata": {
        "id": "UT8q8fDB1VT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find peaks\n",
        "def detect_peaks(signal, threshold=None, min_distance=1, prominence=None):\n",
        "    if threshold is None:\n",
        "        # Set threshold as a percentage of the maximum peak\n",
        "        max_peak = np.max(signal)\n",
        "    peaks, _ = find_peaks(signal, height=threshold, distance=min_distance, prominence=prominence)\n",
        "    return peaks"
      ],
      "metadata": {
        "id": "DempfVXak2Da"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### peak values extracting function"
      ],
      "metadata": {
        "id": "ghd-Cz3n0dKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_peaks(X, Y, peak_range):\n",
        "  # pandas to numpy\n",
        "  peak_X = X.to_numpy()\n",
        "  peak_Y = Y.to_numpy()\n",
        "  # extract the peak value index\n",
        "  index = list(peak_Y).index(max(peak_Y))\n",
        "\n",
        "  # temp saving\n",
        "  extracted_x = []\n",
        "  extracted_y = []\n",
        "\n",
        "  # extract the values from highest peak, and surrounding peaks\n",
        "  for k in range(peak_range):\n",
        "    # index labeling\n",
        "    i = k + 1\n",
        "    p_temp_index = index + i\n",
        "    m_temp_index = index - i\n",
        "    # extract y\n",
        "    extracted_y.append(peak_Y[p_temp_index])\n",
        "    extracted_y.append(peak_Y[m_temp_index])\n",
        "    # extract X\n",
        "    extracted_x.append(peak_X[p_temp_index])\n",
        "    extracted_x.append(peak_X[m_temp_index])\n",
        "\n",
        "  # extrack the center(highest) value\n",
        "  extracted_x.append(peak_X[index])\n",
        "  extracted_y.append(peak_Y[index])\n",
        "\n",
        "  return extracted_x, extracted_y"
      ],
      "metadata": {
        "id": "UhTkr3NYug7M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H28yFdJS5gPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Function for data with two labels"
      ],
      "metadata": {
        "id": "OlXYonOh8mfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_function(path, peak_range):\n",
        "    extracted_data = []  # Container for extracted peak data\n",
        "    label = []  # Container for labels\n",
        "\n",
        "    folder_list = os.listdir(path)\n",
        "\n",
        "    for folder_name in folder_list:\n",
        "        temp_folder_list = os.listdir(os.path.join(path, folder_name))\n",
        "\n",
        "        for file_name in temp_folder_list:\n",
        "            # Assign label based on folder name\n",
        "            label.append(1 if float(folder_name) > 1.36 else 0)\n",
        "\n",
        "            # Read the data into a Pandas DataFrame\n",
        "            file_path = os.path.join(path, folder_name, file_name)\n",
        "            data = pd.read_csv(file_path, skiprows=24)\n",
        "            data_origin = pd.read_csv(file_path, skiprows=24)\n",
        "\n",
        "            # Restore the removed data (the first data)\n",
        "            a = float(data_origin.columns[0])\n",
        "            b = float(data_origin.columns[1])\n",
        "            data.loc[0] = [a, b]\n",
        "            data.columns = ['Wave_length', 'Amplitude']\n",
        "\n",
        "            # Remove DC offset\n",
        "            dc_removed_Y = remove_dc(data['Amplitude'])\n",
        "\n",
        "            # Detect peaks\n",
        "            min_distance = 100  # Set the minimum distance between peaks\n",
        "            local_maxima_indices = detect_peaks(dc_removed_Y, min_distance=min_distance)\n",
        "            local_maxima_x = data['Wave_length'][local_maxima_indices]\n",
        "            local_maxima_y = dc_removed_Y[local_maxima_indices]\n",
        "\n",
        "            # Extract peaks within the specified range\n",
        "            extracted_x, extracted_y = extract_peaks(local_maxima_x, local_maxima_y, peak_range)\n",
        "\n",
        "            # Concatenate and append to extracted_data\n",
        "            temp = np.concatenate((extracted_x, extracted_y))\n",
        "            extracted_data.append(temp)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    extracted_np_data = np.array(extracted_data)\n",
        "    label = np.array(label)\n",
        "\n",
        "    return extracted_np_data, label\n"
      ],
      "metadata": {
        "id": "9ute-ogV8lN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Function for Data with 5 labels"
      ],
      "metadata": {
        "id": "PyR5ZNEcrP67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_function(path, peak_range):\n",
        "    extracted_data = []  # Container for extracted peak data\n",
        "    label = []  # Container for labels\n",
        "\n",
        "    folder_list = os.listdir(path)\n",
        "\n",
        "    for folder_name in folder_list:\n",
        "        temp_folder_list = os.listdir(os.path.join(path, folder_name))\n",
        "\n",
        "        for file_name in temp_folder_list:\n",
        "            # Assign label based on folder name range\n",
        "            folder_value = float(folder_name)\n",
        "            if folder_value == 1.3:\n",
        "                label.append(0)\n",
        "            elif 1.3 <= folder_value < 1.32:\n",
        "                label.append(1)\n",
        "            elif 1.32 <= folder_value < 1.38:\n",
        "                label.append(2)\n",
        "            elif 1.38 <= folder_value < 1.39:\n",
        "                label.append(3)\n",
        "            elif folder_value >= 1.39:\n",
        "                label.append(4)\n",
        "\n",
        "            # Read the data into a Pandas DataFrame\n",
        "            file_path = os.path.join(path, folder_name, file_name)\n",
        "            data = pd.read_csv(file_path, skiprows=24)\n",
        "            data_origin = pd.read_csv(file_path, skiprows=24)\n",
        "\n",
        "            # Restore the removed data (the first data)\n",
        "            a = float(data_origin.columns[0])\n",
        "            b = float(data_origin.columns[1])\n",
        "            data.loc[0] = [a, b]\n",
        "            data.columns = ['Wave_length', 'Amplitude']\n",
        "\n",
        "            # Remove DC offset\n",
        "            dc_removed_Y = remove_dc(data['Amplitude'])\n",
        "\n",
        "            # Detect peaks\n",
        "            min_distance = 100  # Set the minimum distance between peaks\n",
        "            local_maxima_indices = detect_peaks(dc_removed_Y, min_distance=min_distance)\n",
        "            local_maxima_x = data['Wave_length'][local_maxima_indices]\n",
        "            local_maxima_y = dc_removed_Y[local_maxima_indices]\n",
        "\n",
        "            # Extract peaks within the specified range\n",
        "            extracted_x, extracted_y = extract_peaks(local_maxima_x, local_maxima_y, peak_range)\n",
        "\n",
        "            # Concatenate and append to extracted_data\n",
        "            temp = np.concatenate((extracted_x, extracted_y))\n",
        "            extracted_data.append(temp)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    extracted_np_data = np.array(extracted_data)\n",
        "    label_np = np.array(label)\n",
        "\n",
        "    return extracted_np_data, label_np\n"
      ],
      "metadata": {
        "id": "q3ouHJsxrMMY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/new_data/1stSeries_07.05.21'\n",
        "extracted_data, label = final_function(path,12)\n",
        "np.save(\"/content/drive/MyDrive/newdata_1stSeries.npy\", extracted_data)\n",
        "np.save(\"/content/drive/MyDrive/5labels.npy\", label)"
      ],
      "metadata": {
        "id": "KENEB8MFikIL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "wnpkWb7-V2CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Specify the file path\n",
        "data_path = \"/content/drive/MyDrive/newdata_1stSeries.npy\"\n",
        "labels_path = \"/content/drive/MyDrive/5labels.npy\"\n",
        "\n",
        "# Load the data from the file\n",
        "data = np.load(data_path)\n",
        "labels = np.load(labels_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "# Train the KNN classifier\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the KNN classifier\n",
        "accuracy = knn.score(X_test, y_test)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"-----------------------------------Accuracy before Oversampling-----------------------------------\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate accuracy for each class\n",
        "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "for i, acc in enumerate(class_accuracy):\n",
        "    print(f\"Accuracy for class {i}: {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_jUxTw5uMRD",
        "outputId": "e3be1f4d-e1f6-490b-aa3b-51ecb5c421ff"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------Accuracy before Oversampling-----------------------------------\n",
            "Accuracy: 0.8431372549019608\n",
            "Confusion Matrix:\n",
            "[[ 0  0  0  0  1]\n",
            " [ 1  0  0  0  0]\n",
            " [ 0  2 12  0  1]\n",
            " [ 0  0  1  2  0]\n",
            " [ 0  0  0  2 29]]\n",
            "Accuracy for class 0: 0.00\n",
            "Accuracy for class 1: 0.00\n",
            "Accuracy for class 2: 0.80\n",
            "Accuracy for class 3: 0.67\n",
            "Accuracy for class 4: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from collections import Counter\n",
        "\n",
        "# Apply SMOTE oversampling to the entire dataset\n",
        "X_resampled_smote, y_resampled_smote = SMOTE().fit_resample(data, labels)\n",
        "\n",
        "# Display class distribution after SMOTE oversampling\n",
        "print(\"Class distribution after SMOTE:\", sorted(Counter(y_resampled_smote).items()))\n",
        "\n",
        "# Apply ADASYN oversampling to the entire dataset\n",
        "X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_resample(data, labels)\n",
        "\n",
        "# Display class distribution after ADASYN oversampling\n",
        "print(\"Class distribution after ADASYN:\", sorted(Counter(y_resampled_adasyn).items()))\n",
        "\n",
        "# Split the SMOTE oversampled data into training and testing sets\n",
        "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_resampled_smote, y_resampled_smote, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the ADASYN oversampled data into training and testing sets\n",
        "X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_resampled_adasyn, y_resampled_adasyn, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "\n",
        "# Train the KNN classifier with SMOTE oversampled data\n",
        "knn.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Evaluate the KNN classifier with SMOTE oversampled test data\n",
        "y_pred_smote = knn.predict(X_test_smote)\n",
        "accuracy_smote = accuracy_score(y_test_smote, y_pred_smote)\n",
        "print(\"Overall Accuracy with SMOTE:\", accuracy_smote)\n",
        "\n",
        "# Calculate the confusion matrix with SMOTE oversampled test data\n",
        "conf_matrix_smote = confusion_matrix(y_test_smote, y_pred_smote)\n",
        "print(\"Confusion Matrix with SMOTE:\")\n",
        "print(conf_matrix_smote)\n",
        "\n",
        "# Calculate accuracy for each class with SMOTE\n",
        "class_accuracy_smote = conf_matrix_smote.diagonal() / conf_matrix_smote.sum(axis=1)\n",
        "\n",
        "for i, acc in enumerate(class_accuracy_smote):\n",
        "    print(f\"Accuracy for class {i}: {acc:.2f}\")\n",
        "\n",
        "# Train the KNN classifier with ADASYN oversampled data\n",
        "knn.fit(X_train_adasyn, y_train_adasyn)\n",
        "\n",
        "# Evaluate the KNN classifier with ADASYN oversampled test data\n",
        "y_pred_adasyn = knn.predict(X_test_adasyn)\n",
        "accuracy_adasyn = accuracy_score(y_test_adasyn, y_pred_adasyn)\n",
        "print(\"Overall Accuracy with ADASYN:\", accuracy_adasyn)\n",
        "\n",
        "# Calculate the confusion matrix with ADASYN oversampled test data\n",
        "conf_matrix_adasyn = confusion_matrix(y_test_adasyn, y_pred_adasyn)\n",
        "print(\"Confusion Matrix with ADASYN:\")\n",
        "print(conf_matrix_adasyn)\n",
        "\n",
        "# Calculate accuracy for each class\n",
        "class_accuracy_adasyn = conf_matrix_adasyn.diagonal() / conf_matrix_adasyn.sum(axis=1)\n",
        "\n",
        "for i, acc in enumerate(class_accuracy):\n",
        "    print(f\"Accuracy for class {i}: {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A55CitOS27sb",
        "outputId": "50f82165-41a0-4369-c6b8-5231189046d5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after SMOTE: [(0, 144), (1, 144), (2, 144), (3, 144), (4, 144)]\n",
            "Class distribution after ADASYN: [(0, 148), (1, 144), (2, 145), (3, 144), (4, 144)]\n",
            "Overall Accuracy with SMOTE: 0.8611111111111112\n",
            "Confusion Matrix with SMOTE:\n",
            "[[30  1  0  0  0]\n",
            " [ 9 15  0  0  0]\n",
            " [ 2  2 29  0  0]\n",
            " [ 1  0  0 23  0]\n",
            " [ 1  0  2  2 27]]\n",
            "Accuracy for class 0: 0.97\n",
            "Accuracy for class 1: 0.62\n",
            "Accuracy for class 2: 0.88\n",
            "Accuracy for class 3: 0.96\n",
            "Accuracy for class 4: 0.84\n",
            "Overall Accuracy with ADASYN: 0.8896551724137931\n",
            "Confusion Matrix with ADASYN:\n",
            "[[30  0  0  0  0]\n",
            " [ 5 24  0  0  0]\n",
            " [ 2  2 25  0  0]\n",
            " [ 0  0  1 22  0]\n",
            " [ 1  0  2  3 28]]\n",
            "Accuracy for class 0: 0.00\n",
            "Accuracy for class 1: 0.00\n",
            "Accuracy for class 2: 0.80\n",
            "Accuracy for class 3: 0.67\n",
            "Accuracy for class 4: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming you have your features in 'data' and labels/targets in 'labels'\n",
        "\n",
        "# Apply ADASYN oversampling to the entire dataset\n",
        "X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_resample(data, labels)\n",
        "\n",
        "# Display class distribution after ADASYN oversampling\n",
        "print(\"Class distribution after ADASYN:\", sorted(Counter(y_resampled_adasyn).items()))\n",
        "\n",
        "# Split the ADASYN oversampled data into training and testing sets\n",
        "X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_resampled_adasyn, y_resampled_adasyn, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the Decision Tree Regressor with ADASYN oversampled data\n",
        "dt_regressor.fit(X_train_adasyn, y_train_adasyn)\n",
        "\n",
        "# Evaluate the Decision Tree Regressor with ADASYN oversampled test data\n",
        "y_pred_adasyn_dt = dt_regressor.predict(X_test_adasyn)\n",
        "r2_adasyn_dt = r2_score(y_test_adasyn, y_pred_adasyn_dt)\n",
        "mse_adasyn_dt = mean_squared_error(y_test_adasyn, y_pred_adasyn_dt)\n",
        "print(\"R-squared with Decision Tree Regressor:\", r2_adasyn_dt)\n",
        "print(\"Mean Squared Error with Decision Tree Regressor:\", mse_adasyn_dt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll0qY1FQFOHq",
        "outputId": "7f261c2c-cf0d-441a-e793-f1d622b3fee7"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after ADASYN: [(0, 148), (1, 144), (2, 145), (3, 144), (4, 144)]\n",
            "R-squared with Decision Tree Regressor: 0.9675295592977428\n",
            "Mean Squared Error with Decision Tree Regressor: 0.06896551724137931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTjK_8k3EF7K",
        "outputId": "5e0e86f4-4db7-4d5d-c812-699a426ff69a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared with SMOTE: 0.9739659227706269\n",
            "Mean Squared Error with SMOTE: 0.05423263888888889\n",
            "R-squared with ADASYN: 0.9725121484235041\n",
            "Mean Squared Error with ADASYN: 0.05838275862068966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning on Decision Tree"
      ],
      "metadata": {
        "id": "1vqWsSzFpsvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "\n",
        "# Apply ADASYN oversampling to the entire dataset\n",
        "X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_resample(data, labels)\n",
        "\n",
        "# Display class distribution after ADASYN oversampling\n",
        "print(\"Class distribution after ADASYN:\", sorted(Counter(y_resampled_adasyn).items()))\n",
        "\n",
        "# Split the ADASYN oversampled data into training and testing sets\n",
        "X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_resampled_adasyn, y_resampled_adasyn, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree classifier\n",
        "decision_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the Decision Tree classifier with ADASYN oversampled data\n",
        "decision_tree_classifier.fit(X_train_adasyn, y_train_adasyn)\n",
        "\n",
        "# Evaluate the Decision Tree classifier with ADASYN oversampled test data\n",
        "y_pred_adasyn_decision_tree = decision_tree_classifier.predict(X_test_adasyn)\n",
        "accuracy_adasyn_decision_tree = accuracy_score(y_test_adasyn, y_pred_adasyn_decision_tree)\n",
        "print(\"Overall Accuracy with Decision Tree:\", accuracy_adasyn_decision_tree)\n",
        "\n",
        "# Calculate the confusion matrix with ADASYN oversampled test data\n",
        "#conf_matrix_adasyn_decision_tree = confusion_matrix(y_test_adasyn, y_pred_adasyn_decision_tree)\n",
        "#print(\"Confusion Matrix with ADASYN and Decision Tree:\")\n",
        "#print(conf_matrix_adasyn_decision_tree)\n",
        "\n",
        "# Calculate accuracy for each class\n",
        "class_accuracy_adasyn_decision_tree = conf_matrix_adasyn_decision_tree.diagonal() / conf_matrix_adasyn_decision_tree.sum(axis=1)\n",
        "\n",
        "for i, acc in enumerate(class_accuracy_adasyn_decision_tree):\n",
        "    print(f\"Accuracy for class {i}: {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO-uZsrbpI6J",
        "outputId": "4925f365-2f87-49db-b5f8-5cf95cfd21cf"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after ADASYN: [(0, 148), (1, 144), (2, 145), (3, 144), (4, 144)]\n",
            "Overall Accuracy with Decision Tree: 0.9448275862068966\n",
            "Accuracy for class 0: 1.00\n",
            "Accuracy for class 1: 1.00\n",
            "Accuracy for class 2: 0.86\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OWwcbZy76HSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "\n",
        "# Apply ADASYN oversampling to the entire dataset\n",
        "X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_resample(data, labels)\n",
        "\n",
        "# Split the ADASYN oversampled data into training and testing sets\n",
        "X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_resampled_adasyn, y_resampled_adasyn, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting classifier\n",
        "gbm_classifier = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting classifier with ADASYN oversampled data\n",
        "gbm_classifier.fit(X_train_adasyn, y_train_adasyn)\n",
        "\n",
        "# Evaluate the Gradient Boosting classifier with ADASYN oversampled test data\n",
        "y_pred_adasyn_gbm = gbm_classifier.predict(X_test_adasyn)\n",
        "accuracy_adasyn_gbm = accuracy_score(y_test_adasyn, y_pred_adasyn_gbm)\n",
        "print(\"Gradient Boosting Classifier:\")\n",
        "print(\"Overall Accuracy :\", accuracy_adasyn_gbm)\n",
        "\n",
        "# Calculate the confusion matrix with ADASYN oversampled test data\n",
        "#conf_matrix_adasyn_gbm = confusion_matrix(y_test_adasyn, y_pred_adasyn_gbm)\n",
        "#print(\"Confusion Matrix with ADASYN and Gradient Boosting:\")\n",
        "#print(conf_matrix_adasyn_gbm)\n",
        "\n",
        "# Calculate accuracy for each class\n",
        "class_accuracy_adasyn_gbm = conf_matrix_adasyn_gbm.diagonal() / conf_matrix_adasyn_gbm.sum(axis=1)\n",
        "\n",
        "for i, acc in enumerate(class_accuracy_adasyn_gbm):\n",
        "    print(f\"Accuracy for class {i}: {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9xohqa5rquy",
        "outputId": "81666dbd-1b1c-49e6-f98d-5f49f299102e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classifier:\n",
            "Overall Accuracy : 0.9655172413793104\n",
            "Accuracy for class 0: 1.00\n",
            "Accuracy for class 1: 0.97\n",
            "Accuracy for class 2: 0.97\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "\n",
        "# Apply ADASYN oversampling to the entire dataset\n",
        "X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_resample(data, labels)\n",
        "\n",
        "# Split the ADASYN oversampled data into training and testing sets\n",
        "X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_resampled_adasyn, y_resampled_adasyn, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier with ADASYN oversampled data\n",
        "rf_classifier.fit(X_train_adasyn, y_train_adasyn)\n",
        "\n",
        "# Evaluate the Random Forest classifier with ADASYN oversampled test data\n",
        "y_pred_adasyn_rf = rf_classifier.predict(X_test_adasyn)\n",
        "accuracy_adasyn_rf = accuracy_score(y_test_adasyn, y_pred_adasyn_rf)\n",
        "print(\"Overall Accuracy with Random Forest:\", accuracy_adasyn_rf)\n",
        "\n",
        "# Calculate the confusion matrix with ADASYN oversampled test data\n",
        "#conf_matrix_adasyn_rf = confusion_matrix(y_test_adasyn, y_pred_adasyn_rf)\n",
        "#print(\"Confusion Matrix with ADASYN and Random Forest:\")\n",
        "#print(conf_matrix_adasyn_rf)\n",
        "\n",
        "# Calculate accuracy for each class\n",
        "class_accuracy_adasyn_rf = conf_matrix_adasyn_rf.diagonal() / conf_matrix_adasyn_rf.sum(axis=1)\n",
        "\n",
        "for i, acc in enumerate(class_accuracy_adasyn_rf):\n",
        "    print(f\"Accuracy for class {i}: {acc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lyUA9KqsP5T",
        "outputId": "9c0734b6-0e24-4a75-dfcd-4b68091f1f0e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy with Random Forest: 0.9517241379310345\n",
            "Accuracy for class 0: 1.00\n",
            "Accuracy for class 1: 1.00\n",
            "Accuracy for class 2: 1.00\n",
            "Accuracy for class 3: 1.00\n",
            "Accuracy for class 4: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# Initialize Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor with SMOTE oversampled data\n",
        "rf_reg.fit(X_train_smote, y_train_smote)\n",
        "\n",
        "# Evaluate the Random Forest Regressor with SMOTE oversampled test data\n",
        "y_pred_smote_rf = rf_reg.predict(X_test_smote)\n",
        "r2_smote = r2_score(y_test_smote, y_pred_smote_rf)\n",
        "mse_smote = mean_squared_error(y_test_smote, y_pred_smote_rf)\n",
        "print(\"R-squared with SMOTE:\", r2_smote)\n",
        "print(\"Mean Squared Error with SMOTE:\", mse_smote)\n",
        "\n",
        "# Train the Random Forest Regressor with ADASYN oversampled data\n",
        "rf_reg.fit(X_train_adasyn, y_train_adasyn)\n",
        "\n",
        "# Evaluate the Random Forest Regressor with ADASYN oversampled test data\n",
        "y_pred_adasyn_rf = rf_reg.predict(X_test_adasyn)\n",
        "r2_adasyn = r2_score(y_test_adasyn, y_pred_adasyn_rf)\n",
        "mse_adasyn = mean_squared_error(y_test_adasyn, y_pred_adasyn_rf)\n",
        "print(\"R-squared with ADASYN:\", r2_adasyn)\n",
        "print(\"Mean Squared Error with ADASYN:\", mse_adasyn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0slnTw_srda",
        "outputId": "e8a26500-5721-4b45-cc66-8a78db52f406"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared with SMOTE: 0.9739659227706269\n",
            "Mean Squared Error with SMOTE: 0.05423263888888889\n",
            "R-squared with ADASYN: 0.9725121484235041\n",
            "Mean Squared Error with ADASYN: 0.05838275862068966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from collections import Counter\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Apply ADASYN oversampling to the entire dataset\n",
        "X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_resample(data, labels)\n",
        "\n",
        "# Split the ADASYN oversampled data into training and testing sets\n",
        "X_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_resampled_adasyn, y_resampled_adasyn, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting Regressor with ADASYN oversampled data\n",
        "gb_regressor.fit(X_train_adasyn, y_train_adasyn)\n",
        "\n",
        "# Evaluate the Gradient Boosting Regressor with ADASYN oversampled test data\n",
        "y_pred_adasyn_gb = gb_regressor.predict(X_test_adasyn)\n",
        "r2_adasyn_gb = r2_score(y_test_adasyn, y_pred_adasyn_gb)\n",
        "mse_adasyn_gb = mean_squared_error(y_test_adasyn, y_pred_adasyn_gb)\n",
        "print(\"R-squared Boosting Regressor:\", r2_adasyn_gb)\n",
        "print(\"Mean Squared Error with Gradient Boosting Regressor:\", round(mse_adasyn_gb,2))\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae_adasyn_gb = mean_absolute_error(y_test_adasyn, y_pred_adasyn_gb)\n",
        "print(\"Mean Absolute Error with Gradient Boosting Regressor:\", round(mae_adasyn_gb,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7De2ZtuhFg5v",
        "outputId": "fa52e3c5-d713-4d27-dda5-51f8db0753a7"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Boosting Regressor: 0.9819240111841766\n",
            "Mean Squared Error with Gradient Boosting Regressor: 0.04\n",
            "Mean Absolute Error with Gradient Boosting Regressor: 0.07\n"
          ]
        }
      ]
    }
  ]
}