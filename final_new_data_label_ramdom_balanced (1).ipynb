{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zGU4y9BXQX1G",
        "qfyQqOjXkoUg",
        "KDRuPNco2k04",
        "iz_Oa8kW1l0x",
        "ytv5ry9u1ZCp",
        "UT8q8fDB1VT9",
        "ghd-Cz3n0dKp",
        "UZdjv9Oh5gfR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "4CmlDwMjjj9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7923cb50-92ba-4487-fb83-bc3836b28ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Balancing the dataset\n",
        "# From (4:144 2:72 3:12 1:12 0:11) to (4:144 2:144 3:144 1:144 0:144)\n",
        "# For increase the accuracy\n",
        "# The outputs are include original data and balanced data\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import sklearn\n",
        "from google.colab import drive\n",
        "from scipy.signal import find_peaks\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/data/'\n",
        "\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# first data import"
      ],
      "metadata": {
        "id": "zGU4y9BXQX1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_data = []\n",
        "first_data = pd.read_csv('/content/drive/My Drive/data2/REF.CSV', skiprows=24)\n",
        "first_data_origin = pd.read_csv('/content/drive/My Drive/data2/REF.CSV', skiprows=24)\n",
        "first_a = float(first_data_origin.columns[0])\n",
        "first_b = float(first_data_origin.columns[1])\n",
        "first_data.loc[0] = [first_a, first_b]\n",
        "first_data.columns=['Wave_length', 'Amplitude']\n",
        "first_data['label']=1.3\n",
        "first_np = first_data[['Wave_length', 'Amplitude']].to_numpy()\n",
        "first_np = np.ravel(first_np, order='F')\n",
        "first_np = np.array([first_np])"
      ],
      "metadata": {
        "id": "t9rdQAhBDPGv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# functions"
      ],
      "metadata": {
        "id": "qfyQqOjXkoUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### importing first data function"
      ],
      "metadata": {
        "id": "KDRuPNco2k04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_data_import():\n",
        "  # import the first_data\n",
        "  # first_data making\n",
        "  first_data = []\n",
        "  first_data = pd.read_csv('/content/drive/My Drive/data2/REF.CSV', skiprows=24)\n",
        "  first_data_origin = pd.read_csv('/content/drive/My Drive/data2/REF.CSV', skiprows=24)\n",
        "  first_a = float(first_data_origin.columns[0])\n",
        "  first_b = float(first_data_origin.columns[1])\n",
        "  first_data.loc[0] = [first_a, first_b]\n",
        "  first_data.columns=['Wave_length', 'Amplitude']\n",
        "  first_data['label']=1.3\n",
        "  first_np = first_data[['Wave_length', 'Amplitude']].to_numpy()\n",
        "  first_np = np.ravel(first_np, order='F')\n",
        "  first_np = np.array([first_np])\n",
        "\n",
        "  return first_data, first_np"
      ],
      "metadata": {
        "id": "upWEq9PEAww5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### importing whole data and stack function"
      ],
      "metadata": {
        "id": "iz_Oa8kW1l0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stack_data(path):\n",
        "  # the array that combined whole data\n",
        "  stack_np = np.array([[]])\n",
        "  # extract the folder name (it will be the label)\n",
        "  folder_list = os.listdir(path)\n",
        "  # extract each folder name\n",
        "  for folder_name in folder_list:\n",
        "    temp_folder_list = os.listdir(path + folder_name)\n",
        "    # extract each file name\n",
        "    for file_name in temp_folder_list:\n",
        "      # Read the data into a Pandas DataFrame\n",
        "      data = pd.read_csv(path + folder_name + '/' + file_name, skiprows=24)\n",
        "      data_origin = pd.read_csv(path + folder_name + '/' + file_name, skiprows=24)\n",
        "      # generate columns name and restore the removed data(the first data)\n",
        "      a = float(data_origin.columns[0])\n",
        "      b = float(data_origin.columns[1])\n",
        "      data.loc[0] = [a, b]\n",
        "      data.columns=['Wave_length', 'Amplitude']\n",
        "      # make the pandas data to numpy\n",
        "      temp = data[['Wave_length', 'Amplitude']].to_numpy()\n",
        "      # make it flat\n",
        "      temp = np.ravel(temp, order='F')\n",
        "      temp = np.array([temp])\n",
        "      stack_np = np.concatenate((first_np, temp), axis = 1)\n",
        "  return stack_np"
      ],
      "metadata": {
        "id": "YuVeX1GIBMOh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### removing DC function"
      ],
      "metadata": {
        "id": "ytv5ry9u1ZCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing a simple DC removal function\n",
        "def remove_dc(signal):\n",
        "    # Calculate the mean of the signal\n",
        "    signal_mean = np.mean(signal)\n",
        "    # Remove the mean from the signal\n",
        "    dc_removed_signal = signal - signal_mean\n",
        "    return dc_removed_signal"
      ],
      "metadata": {
        "id": "nJrTjRgYmB--"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### finding peaks function"
      ],
      "metadata": {
        "id": "UT8q8fDB1VT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find peaks\n",
        "def detect_peaks(signal, threshold=None, min_distance=1, prominence=None):\n",
        "    if threshold is None:\n",
        "        # Set threshold as a percentage of the maximum peak\n",
        "        max_peak = np.max(signal)\n",
        "    peaks, _ = find_peaks(signal, height=threshold, distance=min_distance, prominence=prominence)\n",
        "    return peaks"
      ],
      "metadata": {
        "id": "DempfVXak2Da"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### peak values extracting function"
      ],
      "metadata": {
        "id": "ghd-Cz3n0dKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_peaks(X, Y, peak_range):\n",
        "  # pandas to numpy\n",
        "  peak_X = X.to_numpy()\n",
        "  peak_Y = Y.to_numpy()\n",
        "  # extract the peak value index\n",
        "  index = list(peak_Y).index(max(peak_Y))\n",
        "\n",
        "  # temp saving\n",
        "  extracted_x = []\n",
        "  extracted_y = []\n",
        "\n",
        "  # extract the values from highest peak, and surrounding peaks\n",
        "  for k in range(peak_range):\n",
        "    # index labeling\n",
        "    i = k + 1\n",
        "    p_temp_index = index + i\n",
        "    m_temp_index = index - i\n",
        "    # extract y\n",
        "    extracted_y.append(peak_Y[p_temp_index])\n",
        "    extracted_y.append(peak_Y[m_temp_index])\n",
        "    # extract X\n",
        "    extracted_x.append(peak_X[p_temp_index])\n",
        "    extracted_x.append(peak_X[m_temp_index])\n",
        "\n",
        "  # extrack the center(highest) value\n",
        "  extracted_x.append(peak_X[index])\n",
        "  extracted_y.append(peak_Y[index])\n",
        "\n",
        "  return extracted_x, extracted_y"
      ],
      "metadata": {
        "id": "UhTkr3NYug7M"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### final function"
      ],
      "metadata": {
        "id": "UZdjv9Oh5gfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_function(path, peak_range):\n",
        "  # extracted peak amplitudes and wavelength array\n",
        "  extracted_data = []\n",
        "  # making label\n",
        "  label = []\n",
        "  # the array that combined whole data\n",
        "  stack_np = np.array([[]])\n",
        "  # extract the folder name (it will be the label)\n",
        "  folder_list = os.listdir(path)\n",
        "  # extract each folder name\n",
        "  for folder_name in folder_list:\n",
        "    temp_folder_list = os.listdir(path + folder_name)\n",
        "    # extract each file name\n",
        "    for file_name in temp_folder_list:\n",
        "      # making the label\n",
        "      if float(folder_name) == 1.3:\n",
        "        label.append(0)\n",
        "      elif 1.3 <= float(folder_name) < 1.32:\n",
        "        label.append(1)\n",
        "      elif 1.32 <= float(folder_name) < 1.38:\n",
        "        label.append(2)\n",
        "      elif 1.38 <= float(folder_name) < 1.39:\n",
        "        label.append(3)\n",
        "      elif float(folder_name) >= 1.39:\n",
        "        label.append(4)\n",
        "\n",
        "      # Read the data into a Pandas DataFrame\n",
        "      data = pd.read_csv(path + folder_name + '/' + file_name, skiprows=24)\n",
        "      data_origin = pd.read_csv(path + folder_name + '/' + file_name, skiprows=24)\n",
        "      # generate columns name and restore the removed data(the first data)\n",
        "      a = float(data_origin.columns[0])\n",
        "      b = float(data_origin.columns[1])\n",
        "      data.loc[0] = [a, b]\n",
        "      data.columns=['Wave_length', 'Amplitude']\n",
        "\n",
        "      # DC removed\n",
        "      dc_removed_Y = remove_dc(data['Amplitude'])\n",
        "\n",
        "      # dececting peaks\n",
        "      min_distance = 100  # Set the minimum distance between peaks\n",
        "      local_maxima_indices = detect_peaks(dc_removed_Y, min_distance=min_distance)\n",
        "      local_maxima_x = data['Wave_length'][local_maxima_indices]\n",
        "      local_maxima_y = dc_removed_Y[local_maxima_indices]\n",
        "\n",
        "      # extract_peaks\n",
        "      extracted_x, extracted_y = extract_peaks(local_maxima_x, local_maxima_y, peak_range)\n",
        "      # saving\n",
        "      temp = []\n",
        "      temp.extend(extracted_x)\n",
        "      temp.extend(extracted_y)\n",
        "      extracted_data.append(temp)\n",
        "\n",
        "  label_np = np.array(label)\n",
        "  extracted_np_data = np.array(extracted_data)\n",
        "  # data = [wavelength, amplitude] -> the number of each parameter is related with the peak range\n",
        "  return extracted_np_data, label_np"
      ],
      "metadata": {
        "id": "H28yFdJS5gPX"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Over Sampling: Balanced Label vs Original Label"
      ],
      "metadata": {
        "id": "sJxTegD1rQyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data and labels\n",
        "data, label = final_function(path, 2)\n",
        "print ('Original_Data')\n",
        "print (data.shape)\n",
        "print (label)\n",
        "\n",
        "counter = Counter(label)\n",
        "\n",
        "# Print the count of each value\n",
        "for value, count in counter.items():\n",
        "    print(f\"{value}: {count}\")\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/extracted_data/data.npy\", data)\n",
        "np.save(\"/content/drive/MyDrive/extracted_data/label.npy\", label)\n",
        "\n",
        "print ('-----------------------')\n",
        "\n",
        "\n",
        "# Function to balance the dataset using Random Oversampling\n",
        "def balance_dataset(X, y):\n",
        "    # Create an instance of RandomOverSampler\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    # Resample the dataset\n",
        "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_data, balanced_label = balance_dataset(data, label)\n",
        "\n",
        "# Check the shape of the balanced dataset\n",
        "print ('Balanced_Data')\n",
        "print (balanced_data.shape)\n",
        "print (balanced_label)\n",
        "\n",
        "counter = Counter(balanced_label)\n",
        "\n",
        "# Print the count of each value\n",
        "for value, count in counter.items():\n",
        "    print(f\"{value}: {count}\")\n",
        "\n",
        "# Save the balanced dataset as NumPy arrays\n",
        "np.save(\"/content/drive/MyDrive/extracted_data/balanced_data.npy\", balanced_data)\n",
        "np.save(\"/content/drive/MyDrive/extracted_data/balanced_label.npy\", balanced_label)\n"
      ],
      "metadata": {
        "id": "pEi02DpdqLDP",
        "outputId": "ade1bb0f-5539-44aa-a1c2-eed9946cf38c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original_Data\n",
            "(251, 10)\n",
            "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
            "4: 144\n",
            "2: 72\n",
            "3: 12\n",
            "1: 12\n",
            "0: 11\n",
            "-----------------------\n",
            "Balanced_Data\n",
            "(720, 10)\n",
            "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            "4: 144\n",
            "2: 144\n",
            "3: 144\n",
            "1: 144\n",
            "0: 144\n"
          ]
        }
      ]
    }
  ]
}